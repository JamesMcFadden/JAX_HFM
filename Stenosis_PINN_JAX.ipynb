{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stenosis_PINN_JAX.ipynb","provenance":[{"file_id":"1WPFJrUmjrHjKYmsCBDgLE70VCiqBAYGC","timestamp":1651988584063},{"file_id":"1y-W0Y7U9a32CPqIB5e2qwLzEIxZ2OcZY","timestamp":1651788024504},{"file_id":"1aPw09kC4udKuKeYxBisA8QJysIMzPrbb","timestamp":1651718992564}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Jax Version of Stenosis2D"],"metadata":{"id":"Bd3_K5tKH8vS"}},{"cell_type":"markdown","source":["## Notes\n","* It runs fine, but the error is the same throughout all runs ==> My thought is that it might has something to do with the learning rate? Or how we propagates the data?\n","\n","\n","Suggestions\n","* Look at normalization\n","* Look at updating Reynolds and Peclet number"],"metadata":{"id":"kDtFfCwifBZW"}},{"cell_type":"markdown","source":["## Import of libraries"],"metadata":{"id":"9dmd8RPxAmdM"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3KoaBluIFpl","executionInfo":{"status":"ok","timestamp":1652211094129,"user_tz":240,"elapsed":1899,"user":{"displayName":"James McFadden","userId":"07406441133091006051"}},"outputId":"a33ba603-e7aa-41c7-8b8a-d9b0a92816e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/ENM531\\ Project/\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OK-QaN3TIJBu","executionInfo":{"status":"ok","timestamp":1652211094130,"user_tz":240,"elapsed":14,"user":{"displayName":"James McFadden","userId":"07406441133091006051"}},"outputId":"b53f7188-9459-4ef5-f861-3b7ca22ca764"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1rxnV06iwKeFXatMhja60OvGThJkZnQHG/ENM531 Project\n","/content/drive/.shortcut-targets-by-id/1rxnV06iwKeFXatMhja60OvGThJkZnQHG/ENM531 Project\n"]}]},{"cell_type":"code","source":["import jax.numpy as np\n","import numpy as onp\n","from jax import random, jit, vmap, grad, device_put, jacrev, hessian\n","from jax.experimental.optimizers import optimizer, make_schedule, exponential_decay\n","import scipy.io\n","\n","import itertools\n","from functools import partial\n","from tqdm import trange\n","from torch.utils import data\n","import matplotlib.pyplot as plt\n","\n","# from james_utilities import MLP_pde, Navier_Stokes_2D, Strain_Rate_2D, mean_squared_error, relative_error, fwd_gradients\n","from JAX_utilities import MLP_pde, Navier_Stokes_2D, Strain_Rate_2D, mean_squared_error, relative_error, fwd_gradients"],"metadata":{"id":"gXdOVnJcH_c4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize optimizer, data generator, and HFM class"],"metadata":{"id":"LKpllZP0AxJt"}},{"cell_type":"code","source":["@optimizer\n","def sgd(step_size):\n","    step_size = make_schedule(step_size)\n","    def init(x0):\n","        return x0\n","    def update(i, g, x):\n","        return x - step_size(i) * g\n","    def get_params(x):\n","        return x\n","    return init, update, get_params\n","\n","@optimizer\n","def adam(step_size, b1=0.9, b2=0.999, eps=1e-8):\n","    step_size = make_schedule(step_size)\n","    def init(x0):\n","        m0 = np.zeros_like(x0)\n","        v0 = np.zeros_like(x0)\n","        return x0, m0, v0\n","    def update(i, g, state):\n","        x, m, v = state\n","        m = (1 - b1) * g + b1 * m  # First  moment estimate.\n","        v = (1 - b2) * np.square(g) + b2 * v  # Second moment estimate.\n","        mhat = m / (1 - np.asarray(b1, m.dtype) ** (i + 1))  # Bias correction.\n","        vhat = v / (1 - np.asarray(b2, m.dtype) ** (i + 1))\n","        x = x - step_size(i) * mhat / (np.sqrt(vhat) + eps)\n","        return x, m, v\n","    def get_params(state):\n","        x, _, _ = state\n","        return x\n","    return init, update, get_params"],"metadata":{"id":"ChBqSXW-IfvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HFM:\n","    # notational conventions\n","    # _tf: placeholders for input/output data and points used to regress the equations\n","    # _pred: output of neural network\n","    # _eqns: points used to regress the equations\n","    # _data: input-output data\n","    # _star: predictions\n","\n","    def __init__(self, t_data, x_data, y_data, c_data,\n","                       t_eqns, x_eqns, y_eqns,\n","                       layers, rng_key=random.PRNGKey(19)):\n","\n","        # MLP init and apply functions\n","        self.net_init, self.net_apply = MLP_pde(layers)\n","        params = self.net_init(rng_key)\n","        # self.params = params\n","\n","        # Optimizer initialization and update functions\n","        lr = exponential_decay(1e-3, decay_steps=1000, decay_rate=0.999)\n","        self.opt_init, \\\n","        self.opt_update, \\\n","        self.get_params = adam(lr)\n","        self.opt_state = self.opt_init(params)\n","\n","        # Logger\n","        self.itercount = itertools.count()\n","        self.loss_log = []\n","\n","        # specs\n","        self.layers = layers\n","        \n","        # flow properties\n","        self.Pec = 15.0\n","        self.Rey = 5.0\n","                \n","        # data\n","        [self.t_data, self.x_data, self.y_data, self.c_data] = [t_data, x_data, y_data, c_data]\n","        [self.t_eqns, self.x_eqns, self.y_eqns] = [t_eqns, x_eqns, y_eqns]\n","        \n","        # inputs\n","        [self.t_data_input, self.x_data_input, self.y_data_input, self.c_data_input] = [np.empty((1,),dtype=np.float32) for _ in range(4)]\n","        [self.t_eqns_input, self.x_eqns_input, self.y_eqns_input] = [np.empty((1,),dtype=np.float32) for _ in range(3)]\n","        \n","        # physics \"uninformed\" neural networks\n","    def neural_net(self, params, t, x, y):\n","        inputs = np.stack([t, x, y])\n","        nn = self.net_apply(params, inputs.T)\n","        ##### Low-key not sure about this implementation\n","        return nn.T\n","\n","    def loss(self, params, t_data, x_data, y_data, c_data):\n","\n","\n","        [self.c_data_pred,\n","        self.u_data_pred,\n","        self.v_data_pred,\n","        self.p_data_pred] = self.neural_net(params, self.t_data_input, self.x_data_input, self.y_data_input)\n","\n","        [self.c_eqns_pred,\n","        self.u_eqns_pred,\n","        self.v_eqns_pred,\n","        self.p_eqns_pred] = self.neural_net(params, self.t_eqns_input,\n","                                                      self.x_eqns_input,\n","                                                      self.y_eqns_input)\n","        \n","        [self.e1_eqns_pred,\n","        self.e2_eqns_pred,\n","        self.e3_eqns_pred,\n","        self.e4_eqns_pred] = self.Navier_Stokes_2D(self.c_eqns_pred.reshape(-1,1),\n","                                              self.u_eqns_pred.reshape(-1,1),\n","                                              self.v_eqns_pred.reshape(-1,1),\n","                                              self.p_eqns_pred.reshape(-1,1),\n","                                              self.t_eqns_input.reshape(-1,1),\n","                                              self.x_eqns_input.reshape(-1,1),\n","                                              self.y_eqns_input.reshape(-1,1),\n","                                              self.Pec,\n","                                              self.Rey,\n","                                              params)\n","        \n","        # [self.eps11dot_eqns_pred,\n","        # self.eps12dot_eqns_pred,\n","        # self.eps22dot_eqns_pred] = Strain_Rate_2D(self.u_eqns_pred,\n","        #                                           self.v_eqns_pred,\n","        #                                           self.x_eqns_input,\n","        #                                           self.y_eqns_input,\n","        #                                           self.t_eqns_input,\n","        #                                           params)\n","        \n","        # loss ==> pulled from utilities function of mean_square_error\n","        loss = np.mean(np.square(np.subtract(self.c_data_pred, np.array(self.c_data_input)))) + \\\n","                np.mean(np.square(np.subtract(self.e1_eqns_pred,np.array([0.0])))) + \\\n","                np.mean(np.square(np.subtract(self.e2_eqns_pred,np.array([0.0])))) + \\\n","                np.mean(np.square(np.subtract(self.e3_eqns_pred,np.array([0.0])))) + \\\n","                np.mean(np.square(np.subtract(self.e4_eqns_pred,np.array([0.0]))))\n","        return loss\n","        \n","    def Navier_Stokes_2D(self, c, u, v, p, t, x, y, Pec, Rey, params):\n","    \n","        Y = np.concatenate([c, u, v, p], 1) # (1,4)\n","        batch = t, x, y\n","        \n","        #Y_t = fwd_gradients(params, batch, Y, 1)\n","        #Y_x = fwd_gradients(params, batch, Y, 2)\n","        #Y_y = fwd_gradients(params, batch, Y, 3)\n","        #Y_xx = fwd_gradients(params, batch, Y_x, 2)\n","        #Y_yy = fwd_gradients(params, batch, Y_y, 3)\n","        Y_t, Y_x, Y_y = jacrev(self.neural_net, (1, 2, 3))(params, t, x, y)\n","        Y_xx = hessian(self.neural_net, 2)(params, t, x, y)\n","        Y_yy = hessian(self.neural_net, 3)(params, t, x, y)\n","\n","        # Outputs from hessian and jacrev were (4,1,1,1,1,1,1) or something like that\n","        # so we squeeze to make (4,) and reshape to get (1,4)\n","        Y_t = np.squeeze(Y_t).reshape(1,-1)\n","        Y_x = np.squeeze(Y_x).reshape(1,-1)\n","        Y_y = np.squeeze(Y_y).reshape(1,-1)\n","        Y_xx = np.squeeze(Y_xx).reshape(1,-1)\n","        Y_yy = np.squeeze(Y_yy).reshape(1,-1)\n","        \n","        c = Y[:,0:1]\n","        u = Y[:,1:2]\n","        v = Y[:,2:3]\n","        p = Y[:,3:4]\n","        \n","        c_t = Y_t[:,0:1]\n","        u_t = Y_t[:,1:2]\n","        v_t = Y_t[:,2:3]\n","        \n","        c_x = Y_x[:,0:1]\n","        u_x = Y_x[:,1:2]\n","        v_x = Y_x[:,2:3]\n","        p_x = Y_x[:,3:4]\n","        \n","        c_y = Y_y[:,0:1]\n","        u_y = Y_y[:,1:2]\n","        v_y = Y_y[:,2:3]\n","        p_y = Y_y[:,3:4]\n","     \n","        c_xx = Y_xx[:,0:1]\n","        u_xx = Y_xx[:,1:2]\n","        v_xx = Y_xx[:,2:3]\n","        \n","        c_yy = Y_yy[:,0:1]\n","        u_yy = Y_yy[:,1:2]\n","        v_yy = Y_yy[:,2:3]\n","        \n","        e1 = c_t + (u*c_x + v*c_y) - (1.0/Pec)*(c_xx + c_yy)\n","        e2 = u_t + (u*u_x + v*u_y) + p_x - (1.0/Rey)*(u_xx + u_yy) \n","        e3 = v_t + (u*v_x + v*v_y) + p_y - (1.0/Rey)*(v_xx + v_yy)\n","        e4 = u_x + v_y\n","        \n","        return e1, e2, e3, e4\n","\n","    def Gradient_Velocity_2D(self, u, v, x, y, t, params):\n","        \n","        Y = np.concatenate([u, v], 1)\n","        batch = t, x, y\n","        \n","        # Y_x = fwd_gradients(params, batch, Y, 1)\n","        # Y_y = fwd_gradients(params, batch, Y, 2)\n","        Y_x, Y_y = jacrev(self.neural_net, (2, 3))(params, t, x, y)\n","        \n","        u_x = Y_x[:,0:1]\n","        v_x = Y_x[:,1:2]\n","        \n","        u_y = Y_y[:,0:1]\n","        v_y = Y_y[:,1:2]\n","        \n","        return [u_x, v_x, u_y, v_y]\n","\n","    def Strain_Rate_2D(self, u, v, x, y, t, params):\n","        \n","        [u_x, v_x, u_y, v_y] = self.Gradient_Velocity_2D(u, v, x, y, t, params)\n","        \n","        eps11dot = u_x\n","        eps12dot = 0.5*(v_x + u_y)\n","        eps22dot = v_y\n","        \n","        return [eps11dot, eps12dot, eps22dot]\n","\n","\n","    @partial(jit, static_argnums=(0,))\n","    def step(self, i, opt_state, batch, t_data, x_data, y_data, c_data):\n","        params = self.get_params(opt_state)\n","        gradients = grad(self.loss)(params, t_data, x_data, y_data, c_data)\n","        return self.opt_update(i, gradients, opt_state)\n","\n","    def train(self, dataset, t_data, x_data, y_data, c_data, nIter = 10):\n","        data = iter(dataset)\n","        pbar = trange(nIter)\n","        # Main training loop\n","        for it in pbar:\n","            # Run one gradient descent update\n","            batch = np.squeeze(np.asarray(next(data))).T\n","            self.opt_state = self.step(next(self.itercount), self.opt_state, batch, t_data, x_data, y_data, c_data) \n","            if it % 50 == 0:\n","                # Logger\n","                params = self.get_params(self.opt_state)\n","                loss = self.loss(params, t_data, x_data, y_data, c_data)\n","                self.loss_log.append(loss)\n","                pbar.set_postfix({'Loss': loss})\n","\n","    def get_logs(self):\n","      return self.loss_log\n","    \n","    @partial(jit, static_argnums=(0,))\n","    def predict(self, params, t, x, y):\n","        inputs = np.stack([t, x, y])\n","        outputs = self.net_apply(params, inputs.T)\n","        return outputs\n","    \n","    # def predict_eps_dot(self, t_star, x_star, y_star):\n","                \n","    #     [self.eps11dot_eqns_pred,\n","    #      self.eps12dot_eqns_pred,\n","    #      self.eps22dot_eqns_pred] = self.Strain_Rate_2D(self.u_eqns_pred,\n","    #                                                self.v_eqns_pred,\n","    #                                                self.x_eqns_input,\n","    #                                                self.y_eqns_input)\n","    #     tf_dict = {self.t_eqns_tf: t_star, self.x_eqns_tf: x_star, self.y_eqns_tf: y_star}\n","        \n","    #     eps11dot_star = self.sess.run(self.eps11dot_eqns_pred, tf_dict)\n","    #     eps12dot_star = self.sess.run(self.eps12dot_eqns_pred, tf_dict)\n","    #     eps22dot_star = self.sess.run(self.eps22dot_eqns_pred, tf_dict)\n","        \n","    #     return eps11dot_star, eps12dot_star, eps22dot_star"],"metadata":{"id":"FsKxAFmuIP_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataGenerator(data.Dataset):\n","    def __init__(self, T, X, Y, \n","                 norm_const=((0.0, 1.0), (0.0, 1.0), (0.0, 1.0)), \n","                 batch_size=128, \n","                 rng_key=random.PRNGKey(1234)):\n","        'Initialization'\n","        self.T = T\n","        self.X = X\n","        self.Y = Y\n","        self.N = Y.shape[0]\n","        self.norm_const = norm_const\n","        self.batch_size = batch_size\n","        self.key = rng_key\n","\n","    @partial(jit, static_argnums=(0,))\n","    def __data_generation(self, key, T, X, Y):\n","        'Generates data containing batch_size samples'\n","        (mu_T, sigma_T), (mu_X, sigma_X), (mu_Y, sigma_Y) = self.norm_const\n","        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n","        T_selected = T[idx,:]\n","        X_selected = X[idx,:]\n","        Y_selected = Y[idx,:]\n","        T_norm = (T_selected - mu_T)/sigma_T\n","        X_norm = (X_selected - mu_X)/sigma_X\n","        Y_norm = (Y_selected - mu_Y)/sigma_Y\n","        return T_norm, X_norm, Y_norm\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        self.key, subkey = random.split(self.key)\n","        T_norm, X_norm, Y_norm = self.__data_generation(self.key, self.T, self.X, self.Y)\n","        return T_norm, X_norm, Y_norm"],"metadata":{"id":"9NNrqJ2gwm5D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initial data and layers setup"],"metadata":{"id":"7HFUyE789cjH"}},{"cell_type":"code","source":["# layers = [3] + 10*[4*50] + [4]\n","# layers = [3] + 5*[4*10] + [4] # Instead of 10 layer DNN with 50 neurons per layer\n","# Inputs are (t,x,y) and outputs are (c,u,v,p)\n","layers = [3, 40, 40, 40, 40, 40, 4]\n","\n","# Load Data\n","data = scipy.io.loadmat('./HFM/Data/Stenosis2D.mat')\n","      \n","t_star = data['t_star'] # T x 1\n","x_star = data['x_star'] # N x 1\n","y_star = data['y_star'] # N x 1\n","\n","T = t_star.shape[0]\n","N = x_star.shape[0]\n","\n","U_star = data['U_star'] # N x T\n","V_star = data['V_star'] # N x T\n","P_star = data['P_star'] # N x T\n","C_star = data['C_star'] # N x T    \n","        \n","# Rearrange Data \n","T_star = np.tile(t_star, (1,N)).T # N x T\n","X_star = np.tile(x_star, (1,T)) # N x T\n","Y_star = np.tile(y_star, (1,T)) # N x T "],"metadata":{"id":"LOJL_mKZ8vco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data['C_star'].shape)\n","print(data['P_star'].shape)\n","print(data['U_star'].shape)\n","print(data['V_star'].shape)\n","print(data['t_star'].shape)\n","print(data['x_star'].shape)\n","print(data['y_star'].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGEyEopU81Ex","executionInfo":{"status":"ok","timestamp":1652211105858,"user_tz":240,"elapsed":9,"user":{"displayName":"James McFadden","userId":"07406441133091006051"}},"outputId":"030bcf31-73b2-4ca4-b9cf-2d6c55d0bfc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(52737, 201)\n","(52737, 201)\n","(52737, 201)\n","(52737, 201)\n","(201, 1)\n","(52737, 1)\n","(52737, 1)\n"]}]},{"cell_type":"markdown","source":["## Noiseless Data"],"metadata":{"id":"W1pfo3UP9fKB"}},{"cell_type":"code","source":["T_data = T # int(sys.argv[1])\n","N_data = N # int(sys.argv[2])\n","key = random.PRNGKey(19)\n","idx_t = np.concatenate([np.array([0]), random.choice(key, T-2, (T_data-2,), replace=False)+1, np.array([T-1])])\n","key, subkey = random.split(key)\n","idx_x = random.choice(key, N, (N_data,), replace=False)\n","t_data = np.ravel(T_star[:, idx_t][idx_x,:])[:, None]       \n","x_data = np.ravel(X_star[:, idx_t][idx_x,:])[:, None] \n","y_data = np.ravel(Y_star[:, idx_t][idx_x,:])[:, None] \n","c_data = np.ravel(C_star[:, idx_t][idx_x,:])[:, None] \n","    \n","T_eqns = T\n","N_eqns = N\n","key, subkey = random.split(key)\n","idx_t = np.concatenate([np.array([0]), random.choice(key, T-2, (T_eqns-2,), replace=False)+1, np.array([T-1])])\n","key, subkey = random.split(key)\n","idx_x = random.choice(key, N, (N_eqns,), replace=False)\n","t_eqns = np.ravel(T_star[:, idx_t][idx_x,:])[:, None] \n","x_eqns = np.ravel(X_star[:, idx_t][idx_x,:])[:, None] \n","y_eqns = np.ravel(Y_star[:, idx_t][idx_x,:])[:, None] \n","\n","norm_const = ((np.mean(t_eqns), np.std(t_eqns)), (np.mean(x_eqns), np.std(x_eqns)), (np.mean(y_eqns), np.std(y_eqns)))\n","dataset = DataGenerator(t_eqns, x_eqns, y_eqns, norm_const=norm_const, batch_size=1000)"],"metadata":{"id":"h4fKHf3A9JU0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define and train model"],"metadata":{"id":"142A2ar29osM"}},{"cell_type":"code","source":["# Define model\n","model = HFM(t_data, x_data, y_data, c_data,\n","            t_eqns, x_eqns, y_eqns,\n","            layers, rng_key=random.PRNGKey(19))"],"metadata":{"id":"pWv2CR5X9p90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.train(dataset, t_data, x_data, y_data, c_data, 50000) # train model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-IKQMFR9r5k","outputId":"9cb9420b-cea5-4a81-f0d8-a35a07d8fb37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 40948/50000 [5:15:43<1:09:06,  2.18it/s, Loss=2.801248e-13]"]}]},{"cell_type":"markdown","source":["## Shear stress calculation and saving shear stress data"],"metadata":{"id":"iUHyxmZaBHA6"}},{"cell_type":"code","source":["# Calculate shear stress\n","# Shear = np.zeros((300,t_star.shape[0]))\n","\n","# for snap in range(0,t_star.shape[0]):\n","    \n","#     x1_shear = np.linspace(15,25,100)[:,None]\n","#     x2_shear = np.linspace(25,35,100)[:,None]\n","#     x3_shear = np.linspace(35,55,100)[:,None]\n","\n","#     x_shear = np.concatenate([x1_shear,x2_shear,x3_shear], axis=0)\n","\n","#     y1_shear = 0.0*x1_shear\n","#     y2_shear = np.sqrt(25.0 - (x2_shear - 30.0)**2)\n","#     y3_shear = 0.0*x3_shear\n","\n","#     y_shear = np.concatenate([y1_shear,y2_shear,y3_shear], axis=0)\n","        \n","#     t_shear = T_star[0,snap] + 0.0*x_shear\n","    \n","#     eps11_dot_shear, eps12_dot_shear, eps22_dot_shear = model.predict_eps_dot(t_shear, x_shear, y_shear)\n","    \n","#     nx1_shear = 0.0*x1_shear\n","#     nx2_shear = 6.0 - x2_shear/5.0\n","#     nx3_shear = 0.0*x3_shear\n","    \n","#     nx_shear = np.concatenate([nx1_shear,nx2_shear,nx3_shear], axis=0)\n","    \n","#     ny1_shear = -1.0 + 0.0*y1_shear\n","#     ny2_shear = -y2_shear/5.0\n","#     ny3_shear = -1.0 + 0.0*y3_shear\n","    \n","#     ny_shear = np.concatenate([ny1_shear,ny2_shear,ny3_shear], axis=0)\n","    \n","#     shear_x = 2.0*(1.0/5.0)*(eps11_dot_shear*nx_shear + eps12_dot_shear*ny_shear)\n","#     shear_y = 2.0*(1.0/5.0)*(eps12_dot_shear*nx_shear + eps22_dot_shear*ny_shear)\n","    \n","#     shear = np.sqrt(shear_x**2 + shear_y**2)\n","    \n","#     Shear[:,snap] = np.ravel(shear)"],"metadata":{"id":"eZ1T0ngn9z79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !ls"],"metadata":{"id":"JJLMfpwN97cM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save shear data\n","# scipy.io.savemat('./Results/Stenosis2D_Pec_Re_shear_results_%s.mat' %(time.strftime('%d_%m_%Y')),\n","#                   {'Shear':Shear, 'x_shear':x_shear})"],"metadata":{"id":"ZgFy5iZU9_w6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Error calculation and saving prediction results"],"metadata":{"id":"6sLB3pWEfmD0"}},{"cell_type":"code","source":["# Test Data\n","snap = np.array([55])\n","t_test = T_star[:,snap]\n","x_test = X_star[:,snap]\n","y_test = Y_star[:,snap]\n","\n","c_test = C_star[:,snap]\n","u_test = U_star[:,snap]\n","v_test = V_star[:,snap]\n","p_test = P_star[:,snap]\n","\n","# Prediction\n","opt_params = model.get_params(model.opt_state)\n","pred = model.predict(opt_params, t_test, x_test, y_test)\n","c_pred, u_pred, v_pred, p_pred = pred.T\n","# Error\n","# print(p_test)\n","error_c = relative_error(c_pred, c_test)\n","error_c = np.sqrt(np.mean(np.square(c_pred - c_test))/np.mean(np.square(c_pred - np.mean(c_test))))\n","error_u = relative_error(u_pred, u_test)\n","error_u = np.sqrt(np.mean(np.square(u_pred - u_test))/np.mean(np.square(u_pred - np.mean(u_test))))\n","error_v = relative_error(v_pred, v_test)\n","error_v = np.sqrt(np.mean(np.square(v_pred - v_test))/np.mean(np.square(v_pred - np.mean(v_test))))\n","error_p = relative_error(p_pred - np.mean(p_pred), p_test - np.mean(p_test))\n","error_p = np.sqrt(np.mean(np.square(p_pred - np.mean(p_pred) - p_test - np.mean(p_test)))/np.mean(np.square(p_pred - np.mean(p_pred) - np.mean(p_test - np.mean(p_test)))))\n","\n","# print(error_c)\n","print('Error c: %e' % (error_c))\n","print('Error u: %e' % (error_u))\n","print('Error v: %e' % (error_v))\n","print('Error p: %e' % (error_p))"],"metadata":{"id":"TLgKnMgZ-Dng","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1652230455272,"user_tz":240,"elapsed":401,"user":{"displayName":"James McFadden","userId":"07406441133091006051"}},"outputId":"c61afb1e-fcb2-48b8-afaa-c2951b9b1682"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c048b9392e44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msnap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT_star\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnap\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_star\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnap\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_star\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnap\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","source":["import time"],"metadata":{"id":"PNf_yg71l9WX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["C_pred = 0*C_star\n","U_pred = 0*U_star\n","V_pred = 0*V_star\n","P_pred = 0*P_star\n","for snap in range(0,t_star.shape[0]):\n","    t_test = T_star[:,snap:snap+1]\n","    x_test = X_star[:,snap:snap+1]\n","    y_test = Y_star[:,snap:snap+1]\n","    \n","    c_test = C_star[:,snap:snap+1]\n","    u_test = U_star[:,snap:snap+1]\n","    v_test = V_star[:,snap:snap+1]\n","    p_test = P_star[:,snap:snap+1]\n","\n","    # Prediction\n","    opt_params = model.get_params(model.opt_state)\n","    pred = model.predict(opt_params, t_test, x_test, y_test)\n","    c_pred, u_pred, v_pred, p_pred = pred.T\n","    \n","    C_pred[:,snap:snap+1] = c_pred\n","    U_pred[:,snap:snap+1] = u_pred\n","    V_pred[:,snap:snap+1] = v_pred\n","    P_pred[:,snap:snap+1] = p_pred\n","\n","    # Error\n","    # print(p_test)\n","    error_c = relative_error(c_pred, c_test)\n","    error_c = np.sqrt(np.mean(np.square(c_pred - c_test))/np.mean(np.square(c_pred - np.mean(c_test))))\n","    error_u = relative_error(u_pred, u_test)\n","    error_u = np.sqrt(np.mean(np.square(u_pred - u_test))/np.mean(np.square(u_pred - np.mean(u_test))))\n","    error_v = relative_error(v_pred, v_test)\n","    error_v = np.sqrt(np.mean(np.square(v_pred - v_test))/np.mean(np.square(v_pred - np.mean(v_test))))\n","    error_p = relative_error(p_pred - np.mean(p_pred), p_test - np.mean(p_test))\n","    error_p = np.sqrt(np.mean(np.square(p_pred - np.mean(p_pred) - p_test - np.mean(p_test)))/np.mean(np.square(p_pred - np.mean(p_pred) - np.mean(p_test - np.mean(p_test)))))\n","\n","    print('==============')\n","    print('Error c: %e' % (error_c))\n","    print('Error u: %e' % (error_u))\n","    print('Error v: %e' % (error_v))\n","    print('Error p: %e' % (error_p))\n","    print('==============')\n","\n","scipy.io.savemat('./Results/JAX_Stenosis2D_Pec_Re_results_test%s.mat' %(time.strftime('%d_%m_%Y')),\n","                  {'C_pred':C_pred, 'U_pred':U_pred, 'V_pred':V_pred, 'P_pred':P_pred, 'Pec': model.Pec, 'Rey': model.Rey})"],"metadata":{"id":"m3Qb4UIRfpaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"L-U2NwcRev4C"},"execution_count":null,"outputs":[]}]}